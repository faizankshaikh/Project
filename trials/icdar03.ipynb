{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX TITAN X (CNMeM is disabled)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import sys\n",
    "import pylab\n",
    "import random\n",
    "from random import randint, uniform\n",
    "from skimage.util import crop\n",
    "from skimage import transform\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cPickle as pkl\n",
    "from lasagne import layers\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from lasagne import updates\n",
    "import lasagne as nn\n",
    "from theano.tensor.nnet import softmax\n",
    "from scipy.misc import imread, imresize\n",
    "from nolearn.lasagne import NeuralNet, BatchIterator\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "repo_location = '/workspace/.project/project/'\n",
    "data_root = os.path.join(os.path.expanduser('~') + repo_location + 'datasets/')\n",
    "script_root = os.path.join(os.path.expanduser('~') + repo_location + 'scripts/')\n",
    "model_root = os.path.join(os.path.expanduser('~') + repo_location + 'models/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded icdar03\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "train_soup = bs(open(data_root + 'icdar03/train/char/char.xml').read(), 'lxml-xml')\n",
    "test_soup = bs(open(data_root + 'icdar03/test/char/char.xml').read(), 'lxml-xml')\n",
    "\n",
    "X_train = []\n",
    "y_train = []\n",
    "X_test = []\n",
    "y_test = []\n",
    "\n",
    "for image in train_soup('image'):\n",
    "    try:\n",
    "        img = imread(data_root + 'icdar03/train/char/' + image['file'])\n",
    "        X_train.append(img)\n",
    "        y_train.append(image['tag'])\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "for image in test_soup('image'):\n",
    "    try:\n",
    "        img = imread(data_root + 'icdar03/test/char/' + image['file'])\n",
    "        X_test.append(img)\n",
    "        y_test.append(image['tag'])\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    \n",
    "data_train = pd.DataFrame({'image' : X_train, 'label' : y_train})\n",
    "data_test = pd.DataFrame({'image' : X_test, 'label' : y_test})\n",
    "\n",
    "print 'Loaded icdar03'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "icdar03 reshaped and grayscaled\n"
     ]
    }
   ],
   "source": [
    "# Reshape images to 32x32 and convert to grayscale\n",
    "data_train_x = np.zeros((data_train['image'].count(), 1, 32, 32))\n",
    "data_train_y = data_train['label'].values\n",
    "data_test_x = np.zeros((data_test['image'].count(), 1, 32, 32))\n",
    "data_test_y = data_test['label'].values\n",
    "\n",
    "for idx, img in enumerate(data_train['image']):\n",
    "    img = imresize(img, (32, 32))\n",
    "    if len(img.shape) == 3:\n",
    "        data_train_x[idx, ...] = img.dot([0.299, 0.587, 0.144])\n",
    "    else:\n",
    "        data_train_x[idx, ...] = img\n",
    "        \n",
    "for idx, img in enumerate(data_test['image']):\n",
    "    img = imresize(img, (32, 32))\n",
    "    if len(img.shape) == 3:\n",
    "        data_test_x[idx, ...] = img.dot([0.299, 0.587, 0.144])\n",
    "    else:\n",
    "        data_test_x[idx, ...] = img\n",
    "        \n",
    "data_train_x = data_train_x.astype('float32')\n",
    "data_test_x = data_test_x.astype('float32')\n",
    "print 'icdar03 reshaped and grayscaled'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Normalize by MuSigma\n",
    "data_train_x /= data_train_x.std(axis = None)\n",
    "data_train_x -= data_train_x.mean()\n",
    "\n",
    "data_test_x /= data_test_x.std(axis = None)\n",
    "data_test_x -= data_test_x.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6185, 1, 32, 32) (6185,) (5430, 1, 32, 32) (5430,)\n"
     ]
    }
   ],
   "source": [
    "print data_train_x.shape, data_train_y.shape, data_test_x.shape, data_test_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TransIterator(BatchIterator):\n",
    "    def fast_warp(self, img, tf, output_shape, mode='nearest'):\n",
    "        return transform._warps_cy._warp_fast(img, tf.params, output_shape=output_shape, mode=mode)\n",
    "    \n",
    "    def transform(self, Xb, yb):\n",
    "        Xb, yb = super(TransIterator, self).transform(Xb, yb)\n",
    "        \n",
    "        Xb_aug = np.empty(shape = (Xb.shape[0], 1, 32, 32), dtype = 'float32')\n",
    "        yb_aug = yb\n",
    "\n",
    "        # random rotations betweein -5 and 5 degrees\n",
    "        dorotate = randint(-5,5)\n",
    "\n",
    "        # random translations\n",
    "        trans_1 = randint(-3,3)\n",
    "        trans_2 = randint(-3,3)\n",
    "\n",
    "        # random zooms\n",
    "        zoom = uniform(0.8, 1.2)\n",
    "\n",
    "        # shearing\n",
    "        shear_deg = uniform(-10, 10)\n",
    "\n",
    "        # set the transform parameters for skimage.transform.warp\n",
    "        # have to shift to center and then shift back after transformation otherwise\n",
    "        # rotations will make image go out of frame\n",
    "        center_shift   = np.array((32, 32)) / 2. - 0.5\n",
    "        tform_center   = transform.SimilarityTransform(translation=-center_shift)\n",
    "        tform_uncenter = transform.SimilarityTransform(translation=center_shift)\n",
    "\n",
    "        tform_aug = transform.AffineTransform(rotation = np.deg2rad(dorotate),\n",
    "                                              scale =(1/zoom, 1/zoom),\n",
    "                                              shear = np.deg2rad(shear_deg),\n",
    "                                              translation = (trans_1, trans_2))\n",
    "\n",
    "        tform = tform_center + tform_aug + tform_uncenter\n",
    "        \n",
    "        for j in range(Xb.shape[0]):\n",
    "            Xb_aug[j][0] = self.fast_warp(Xb[j][0], tform,\n",
    "                                          output_shape = (32, 32))\n",
    "\n",
    "        return Xb_aug, yb_aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# setting nn \n",
    "net = NeuralNet(\n",
    "    layers = [\n",
    "        ('input', layers.InputLayer),\n",
    "        ('conv1', layers.Conv2DLayer),\n",
    "        ('conv2', layers.Conv2DLayer),\n",
    "        ('pool3', layers.MaxPool2DLayer),\n",
    "        ('dropout4', layers.DropoutLayer),\n",
    "        ('conv5', layers.Conv2DLayer),\n",
    "        ('conv6', layers.Conv2DLayer),\n",
    "        ('pool7', layers.MaxPool2DLayer),\n",
    "        ('dropout8', layers.DropoutLayer),\n",
    "        ('conv9', layers.Conv2DLayer),\n",
    "        ('conv10', layers.Conv2DLayer),\n",
    "        ('dropout12', layers.DropoutLayer),\n",
    "        ('hidden13', layers.DenseLayer),\n",
    "        ('dropout14', layers.DropoutLayer),\n",
    "        ('hidden15', layers.DenseLayer),\n",
    "        ('dropout16', layers.DropoutLayer),\n",
    "        ('output', layers.DenseLayer),\n",
    "    ],\n",
    "\n",
    "    input_shape = (None, 1, 32, 32),\n",
    "    conv1_num_filters = 128, conv1_filter_size = (3, 3),\n",
    "    conv2_num_filters = 128, conv2_filter_size = (3, 3),\n",
    "    pool3_pool_size = (2, 2),\n",
    "    dropout4_p = 0.2,\n",
    "    conv5_num_filters = 256, conv5_filter_size = (3, 3),\n",
    "    conv6_num_filters = 256, conv6_filter_size = (3, 3),\n",
    "    pool7_pool_size = (2, 2),\n",
    "    dropout8_p = 0.2,\n",
    "    conv9_num_filters = 512, conv9_filter_size = (3, 3),\n",
    "    conv10_num_filters = 512, conv10_filter_size = (3, 3),\n",
    "    dropout12_p = 0.2,\n",
    "    hidden13_num_units = 1024,\n",
    "    dropout14_p = 0.5,\n",
    "    hidden15_num_units = 1024,\n",
    "    dropout16_p = 0.5,\n",
    "    output_num_units = 75, output_nonlinearity = softmax,\n",
    "\n",
    "    batch_iterator_train = TransIterator(batch_size = 2500),\n",
    "\n",
    "    update = updates.adam,\n",
    "\n",
    "    use_label_encoder = True,\n",
    "    regression = False,\n",
    "    max_epochs = 300,\n",
    "    verbose = 1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded parameters to layer 'conv1' (shape 128x1x3x3).\n",
      "Loaded parameters to layer 'conv1' (shape 128).\n",
      "Loaded parameters to layer 'conv2' (shape 128x128x3x3).\n",
      "Loaded parameters to layer 'conv2' (shape 128).\n",
      "Loaded parameters to layer 'conv5' (shape 256x128x3x3).\n",
      "Loaded parameters to layer 'conv5' (shape 256).\n",
      "Loaded parameters to layer 'conv6' (shape 256x256x3x3).\n",
      "Loaded parameters to layer 'conv6' (shape 256).\n",
      "Loaded parameters to layer 'conv9' (shape 512x256x3x3).\n",
      "Loaded parameters to layer 'conv9' (shape 512).\n",
      "Loaded parameters to layer 'conv10' (shape 512x512x3x3).\n",
      "Loaded parameters to layer 'conv10' (shape 512).\n",
      "Loaded parameters to layer 'hidden13' (shape 512x1024).\n",
      "Loaded parameters to layer 'hidden13' (shape 1024).\n",
      "Loaded parameters to layer 'hidden15' (shape 1024x1024).\n",
      "Loaded parameters to layer 'hidden15' (shape 1024).\n",
      "Loaded parameters to layer 'output' (shape 1024x75).\n",
      "Loaded parameters to layer 'output' (shape 75).\n",
      "    301       0.24119       1.07617      0.22412      0.77497  12.10s\n",
      "    302       0.33683       1.05447      0.31943      0.79181  12.10s\n",
      "    303       0.30880       1.10565      0.27929      0.77377  12.11s\n",
      "    304       0.29087       1.12611      0.25830      0.77760  12.89s\n",
      "    305       0.40457       1.15368      0.35068      0.76396  12.83s\n",
      "    306       0.25703       1.15764      0.22203      0.76723  12.60s\n",
      "    307       0.40162       1.06146      0.37837      0.77895  12.67s\n",
      "    308       0.29249       1.04930      0.27875      0.78363  12.61s\n",
      "    309       0.56601       1.07411      0.52696      0.77660  12.60s\n",
      "    310       0.31681       1.13917      0.27811      0.76723  12.59s\n",
      "    311       0.54463       1.16610      0.46705      0.76027  12.54s\n",
      "    312       0.34047       1.20180      0.28330      0.74628  12.56s\n",
      "    313       0.33616       1.15850      0.29016      0.76801  12.53s\n",
      "    314       0.38033       1.10235      0.34502      0.78349  12.54s\n",
      "    315       0.41757       1.07602      0.38807      0.76686  12.54s\n",
      "    316       0.42659       1.07471      0.39694      0.76589  12.50s\n",
      "    317       \u001b[36m0.23731\u001b[0m       1.10176      0.21539      0.77087  12.50s\n",
      "    318       0.30413       1.10173      0.27605      0.76823  12.46s\n",
      "    319       0.29608       1.12511      0.26316      0.77355  12.47s\n",
      "    320       0.27690       1.17925      0.23481      0.76645  12.44s\n",
      "    321       0.29757       1.16103      0.25630      0.77575  12.50s\n",
      "    322       \u001b[36m0.21557\u001b[0m       1.15423      0.18676      0.78285  12.41s\n",
      "    323       \u001b[36m0.19880\u001b[0m       1.15966      0.17143      0.78456  12.44s\n",
      "    324       0.24878       1.14755      0.21680      0.78698  12.45s\n",
      "    325       0.25381       1.15665      0.21943      0.78556  12.41s\n",
      "    326       0.36878       1.27354      0.28957      0.77135  12.43s\n",
      "    327       0.40916       1.15913      0.35299      0.78471  12.43s\n",
      "    328       0.24561       1.14497      0.21452      0.77995  12.39s\n",
      "    329       0.44364       1.11250      0.39877      0.78058  12.43s\n",
      "    330       0.27600       1.09490      0.25208      0.77355  12.41s\n",
      "    331       0.40117       1.10609      0.36270      0.77057  12.40s\n",
      "    332       0.23849       1.08809      0.21918      0.77277  12.42s\n",
      "    333       0.24315       1.09821      0.22141      0.77831  12.39s\n",
      "    334       0.77301       1.08365      0.71334      0.78229  12.38s\n",
      "    335       0.48275       1.14530      0.42150      0.76567  12.40s\n",
      "    336       0.25118       1.17042      0.21461      0.76366  12.37s\n",
      "    337       0.37860       1.09956      0.34432      0.77575  12.35s\n",
      "    338       0.38026       1.08764      0.34962      0.76381  12.38s\n",
      "    339       0.30720       1.07772      0.28505      0.76964  12.40s\n",
      "    340       0.34967       1.08315      0.32283      0.77880  12.39s\n",
      "    341       0.26322       1.14528      0.22983      0.78427  12.38s\n",
      "    342       0.28058       1.12475      0.24946      0.78612  12.37s\n",
      "    343       0.46970       1.10426      0.42535      0.78769  12.38s\n",
      "    344       0.35749       1.08916      0.32823      0.79010  12.37s\n",
      "    345       0.30539       1.06475      0.28682      0.77653  12.37s\n",
      "    346       0.35510       1.07966      0.32890      0.75246  12.35s\n",
      "    347       0.30496       1.09575      0.27831      0.74848  12.40s\n",
      "    348       0.20022       1.06634      0.18776      0.77143  12.34s\n",
      "    349       0.21261       1.09158      0.19478      0.78783  12.38s\n",
      "    350       0.22736       1.15236      0.19730      0.79003  12.34s\n",
      "    351       0.25583       1.22084      0.20955      0.77716  12.34s\n",
      "    352       0.31160       1.21365      0.25675      0.78200  12.36s\n",
      "    353       0.33601       1.13600      0.29578      0.77738  12.35s\n",
      "    354       0.27022       1.11719      0.24187      0.78051  12.39s\n",
      "    355       0.50802       1.08510      0.46818      0.78200  12.33s\n",
      "    356       0.40379       1.12978      0.35741      0.76474  12.36s\n",
      "    357       \u001b[36m0.18290\u001b[0m       1.17672      0.15543      0.75373  12.35s\n",
      "    358       0.35891       1.13225      0.31699      0.77192  12.34s\n",
      "    359       0.23614       1.15117      0.20513      0.78129  12.38s\n",
      "    360       0.18381       1.17533      0.15639      0.78051  12.36s\n",
      "    361       0.19001       1.16160      0.16357      0.77255  12.39s\n",
      "    362       0.32909       1.14821      0.28661      0.77675  12.33s\n",
      "    363       0.20948       1.15195      0.18185      0.78237  12.39s\n",
      "    364       \u001b[36m0.18219\u001b[0m       1.15405      0.15787      0.78214  12.37s\n",
      "    365       \u001b[36m0.14141\u001b[0m       1.16191      0.12170      0.78378  12.35s\n",
      "    366       0.21092       1.16613      0.18087      0.78307  12.34s\n",
      "    367       0.21074       1.14577      0.18393      0.78166  12.35s\n",
      "    368       \u001b[36m0.12081\u001b[0m       1.16420      0.10377      0.78464  12.36s\n",
      "    369       0.20145       1.18995      0.16930      0.78214  12.36s\n",
      "    370       0.26898       1.23359      0.21805      0.78285  12.36s\n",
      "    371       0.29242       1.22196      0.23931      0.77377  12.36s\n",
      "    372       0.21670       1.21225      0.17876      0.77619  12.35s\n",
      "    373       0.19159       1.17267      0.16338      0.77612  12.35s\n",
      "    374       0.14361       1.16031      0.12377      0.78144  12.35s\n",
      "    375       0.13532       1.20099      0.11268      0.78293  12.37s\n",
      "    376       0.17712       1.21213      0.14612      0.78705  12.34s\n",
      "    377       0.14571       1.22037      0.11940      0.79096  12.35s\n",
      "    378       0.20486       1.23579      0.16577      0.77895  12.35s\n",
      "    379       0.23864       1.25178      0.19064      0.77355  12.36s\n",
      "    380       0.19896       1.25064      0.15909      0.76567  12.38s\n",
      "    381       0.26941       1.22681      0.21960      0.76347  12.36s\n",
      "    382       0.13668       1.21695      0.11231      0.76752  12.36s\n",
      "    383       0.25211       1.20038      0.21003      0.77768  12.35s\n",
      "    384       0.17980       1.21961      0.14742      0.78002  12.38s\n",
      "    385       0.20378       1.22914      0.16579      0.77987  12.34s\n",
      "    386       0.17763       1.22755      0.14470      0.77377  12.39s\n",
      "    387       0.26316       1.19686      0.21988      0.78464  12.37s\n",
      "    388       0.19527       1.20390      0.16220      0.78471  12.36s\n",
      "    389       0.22556       1.23856      0.18211      0.77909  12.36s\n",
      "    390       0.17909       1.31915      0.13576      0.76317  12.38s\n",
      "    391       0.16626       1.30821      0.12709      0.76544  12.36s\n",
      "    392       0.14312       1.31208      0.10908      0.76645  12.36s\n",
      "    393       \u001b[36m0.11467\u001b[0m       1.32455      0.08657      0.77348  12.33s\n",
      "    394       0.14988       1.28927      0.11625      0.77270  12.37s\n",
      "    395       0.27492       1.22289      0.22481      0.79096  12.36s\n",
      "    396       0.16915       1.24811      0.13553      0.78657  12.40s\n",
      "    397       0.26275       1.24432      0.21116      0.78095  12.34s\n",
      "    398       0.34174       1.20687      0.28317      0.78214  12.35s\n",
      "    399       0.22600       1.26093      0.17923      0.75841  12.36s\n",
      "    400       0.13908       1.30698      0.10641      0.76090  12.39s\n",
      "    401       0.19951       1.29965      0.15351      0.76715  12.35s\n",
      "    402       0.15171       1.28950      0.11765      0.77270  12.40s\n",
      "    403       0.25136       1.25810      0.19980      0.77448  12.34s\n",
      "    404       0.12845       1.27051      0.10110      0.76589  12.36s\n",
      "    405       0.21269       1.20782      0.17610      0.78407  12.35s\n",
      "    406       0.11739       1.21226      0.09684      0.77839  12.37s\n",
      "    407       0.23394       1.22526      0.19093      0.77746  12.33s\n",
      "    408       0.15637       1.27141      0.12299      0.77270  12.34s\n",
      "    409       \u001b[36m0.10456\u001b[0m       1.27599      0.08194      0.77362  12.40s\n",
      "    410       0.26235       1.20822      0.21714      0.77597  12.34s\n",
      "    411       0.15742       1.21614      0.12945      0.78251  12.36s\n",
      "    412       0.13743       1.22962      0.11176      0.78329  12.39s\n",
      "    413       \u001b[36m0.08822\u001b[0m       1.28687      0.06855      0.77704  12.36s\n",
      "    414       0.16718       1.29688      0.12891      0.77846  12.39s\n",
      "    415       0.19349       1.24989      0.15481      0.78207  12.32s\n",
      "    416       0.22292       1.22222      0.18239      0.78512  12.39s\n",
      "    417       \u001b[36m0.08621\u001b[0m       1.23956      0.06955      0.77958  12.37s\n",
      "    418       0.14420       1.26621      0.11388      0.78129  12.37s\n",
      "    419       0.14698       1.26918      0.11581      0.78378  12.38s\n",
      "    420       0.15773       1.25778      0.12540      0.78371  12.36s\n",
      "    421       0.10630       1.29387      0.08215      0.78918  12.36s\n",
      "    422       0.11084       1.31445      0.08432      0.78683  12.37s\n",
      "    423       0.12128       1.33476      0.09087      0.78427  12.37s\n",
      "    424       \u001b[36m0.06713\u001b[0m       1.35327      0.04960      0.78583  12.37s\n",
      "    425       0.16615       1.35318      0.12278      0.78356  12.39s\n",
      "    426       0.20226       1.33557      0.15144      0.78393  12.36s\n",
      "    427       0.14056       1.34598      0.10443      0.78478  12.38s\n",
      "    428       0.32340       1.27772      0.25311      0.78478  12.35s\n",
      "    429       0.32108       1.25294      0.25627      0.78371  12.38s\n",
      "    430       0.12100       1.32642      0.09122      0.77668  12.35s\n",
      "    431       0.16847       1.33397      0.12629      0.78307  12.39s\n",
      "    432       0.17622       1.37089      0.12854      0.77533  12.38s\n",
      "    433       0.12355       1.39247      0.08873      0.77675  12.36s\n",
      "    434       0.39747       1.29654      0.30657      0.76942  12.37s\n",
      "    435       0.23297       1.23353      0.18887      0.77199  12.40s\n",
      "    436       0.12941       1.22413      0.10572      0.76816  12.39s\n",
      "    437       0.19660       1.20768      0.16279      0.77931  12.36s\n",
      "    438       0.09924       1.28509      0.07722      0.78073  12.35s\n",
      "    439       0.19116       1.33587      0.14309      0.77511  12.35s\n",
      "    440       0.15272       1.32903      0.11491      0.78612  12.37s\n",
      "    441       0.16791       1.32305      0.12691      0.78144  12.37s\n",
      "    442       0.08163       1.34954      0.06049      0.77753  12.38s\n",
      "    443       0.15110       1.31822      0.11462      0.78129  12.36s\n",
      "    444       0.14969       1.30691      0.11454      0.77567  12.39s\n",
      "    445       0.12247       1.33909      0.09146      0.77333  12.34s\n",
      "    446       0.15982       1.31684      0.12136      0.78293  12.38s\n",
      "    447       0.22348       1.37393      0.16266      0.77043  12.41s\n",
      "    448       0.16602       1.30436      0.12728      0.77768  12.32s\n",
      "    449       0.13988       1.28627      0.10875      0.77760  12.40s\n",
      "    450       0.34686       1.29128      0.26862      0.76623  12.36s\n",
      "    451       0.13891       1.33354      0.10417      0.76154  12.38s\n",
      "    452       0.19060       1.35496      0.14067      0.76232  12.39s\n",
      "    453       0.15902       1.37746      0.11544      0.75919  12.34s\n",
      "    454       0.17157       1.37727      0.12457      0.75771  12.37s\n",
      "    455       0.17023       1.33446      0.12756      0.76779  12.36s\n",
      "    456       0.15601       1.31196      0.11891      0.76630  12.37s\n",
      "    457       0.10344       1.29884      0.07964      0.77582  12.36s\n",
      "    458       0.09680       1.33717      0.07239      0.78136  12.38s\n",
      "    459       0.17469       1.35926      0.12852      0.77731  12.35s\n",
      "    460       0.21475       1.36013      0.15789      0.76942  12.36s\n",
      "    461       0.11992       1.31308      0.09132      0.77497  12.33s\n",
      "    462       0.17403       1.27596      0.13639      0.78854  12.38s\n",
      "    463       0.13026       1.25642      0.10367      0.78300  12.38s\n",
      "    464       0.10281       1.25201      0.08212      0.79018  12.36s\n",
      "    465       0.24398       1.30289      0.18726      0.77646  12.36s\n",
      "    466       0.07292       1.37601      0.05300      0.77426  12.35s\n",
      "    467       0.15059       1.34902      0.11163      0.77575  12.38s\n",
      "    468       0.12288       1.37567      0.08932      0.77902  12.37s\n",
      "    469       0.10863       1.30884      0.08300      0.77646  12.36s\n",
      "    470       0.24926       1.27918      0.19486      0.78051  12.36s\n",
      "    471       0.26233       1.31266      0.19985      0.76332  12.32s\n",
      "    472       0.17175       1.34970      0.12725      0.75785  12.37s\n",
      "    473       0.24845       1.30970      0.18970      0.77184  12.33s\n",
      "    474       0.18029       1.32934      0.13562      0.77270  12.39s\n",
      "    475       0.17269       1.32642      0.13020      0.77043  12.36s\n",
      "    476       0.17936       1.35136      0.13273      0.76474  12.34s\n",
      "    477       0.16407       1.34790      0.12172      0.75841  12.35s\n",
      "    478       0.17984       1.31715      0.13654      0.76623  12.36s\n",
      "    479       0.08821       1.32977      0.06634      0.77013  12.39s\n",
      "    480       0.16072       1.34123      0.11983      0.76701  12.34s\n",
      "    481       0.12266       1.35434      0.09057      0.76950  12.37s\n",
      "    482       0.13026       1.37590      0.09467      0.77199  12.38s\n",
      "    483       0.25016       1.35541      0.18456      0.76637  12.32s\n",
      "    484       0.07393       1.40781      0.05251      0.76261  12.37s\n",
      "    485       0.09179       1.41155      0.06503      0.76254  12.35s\n",
      "    486       0.09365       1.42793      0.06558      0.77362  12.36s\n",
      "    487       0.24388       1.38450      0.17615      0.77668  12.39s\n",
      "    488       0.13911       1.37361      0.10128      0.77192  12.37s\n",
      "    489       0.15794       1.33327      0.11846      0.77270  12.39s\n",
      "    490       0.18144       1.31281      0.13821      0.77917  12.35s\n",
      "    491       0.08135       1.34470      0.06049      0.76759  12.36s\n",
      "    492       0.09159       1.34086      0.06831      0.76581  12.39s\n",
      "    493       0.25565       1.32866      0.19241      0.77675  12.34s\n",
      "    494       0.09683       1.37930      0.07020      0.77106  12.35s\n",
      "    495       0.18150       1.38922      0.13065      0.76652  12.36s\n",
      "    496       0.09352       1.39005      0.06728      0.76652  12.35s\n",
      "    497       0.09731       1.37117      0.07097      0.78002  12.36s\n",
      "    498       0.10914       1.39811      0.07806      0.77057  12.35s\n",
      "    499       0.07435       1.42616      0.05213      0.77277  12.36s\n",
      "    500       0.10535       1.42366      0.07400      0.77192  12.33s\n",
      "    501       0.10200       1.43103      0.07128      0.77489  12.36s\n",
      "    502       0.15657       1.40078      0.11177      0.76935  12.36s\n",
      "    503       0.15231       1.36613      0.11149      0.77106  12.36s\n",
      "    504       0.08730       1.40255      0.06225      0.77206  12.35s\n",
      "    505       0.26966       1.37561      0.19603      0.77582  12.38s\n",
      "    506       0.08295       1.37422      0.06036      0.77433  12.33s\n",
      "    507       0.10155       1.40049      0.07251      0.77809  12.38s\n",
      "    508       0.25358       1.39018      0.18241      0.77482  12.34s\n",
      "    509       0.22830       1.44050      0.15849      0.77106  12.35s\n",
      "    510       0.17325       1.40918      0.12294      0.76332  12.38s\n",
      "    511       0.14229       1.35348      0.10513      0.75558  12.35s\n",
      "    512       0.19731       1.35017      0.14614      0.75502  12.36s\n",
      "    513       0.28071       1.38793      0.20225      0.75658  12.35s\n",
      "    514       0.32636       1.32268      0.24674      0.77362  12.35s\n",
      "    515       0.10663       1.35165      0.07889      0.77433  12.35s\n",
      "    516       0.09883       1.32426      0.07463      0.76652  12.36s\n",
      "    517       0.16173       1.33206      0.12141      0.76481  12.36s\n",
      "    518       0.17973       1.33337      0.13480      0.76005  12.35s\n",
      "    519       0.16911       1.31418      0.12868      0.76154  12.35s\n",
      "    520       0.13023       1.30357      0.09990      0.77021  12.37s\n",
      "    521       0.10721       1.30327      0.08226      0.77411  12.35s\n",
      "    522       0.14092       1.30305      0.10815      0.77589  12.35s\n",
      "    523       0.09972       1.34511      0.07414      0.77355  12.36s\n",
      "    524       0.13262       1.34643      0.09850      0.77738  12.36s\n",
      "    525       0.08067       1.38429      0.05828      0.77106  12.34s\n",
      "    526       0.13996       1.38291      0.10120      0.77973  12.40s\n",
      "    527       0.07790       1.42406      0.05470      0.77035  12.35s\n",
      "    528       0.07561       1.49155      0.05069      0.76659  12.40s\n",
      "    529       0.16796       1.46991      0.11426      0.76730  12.37s\n",
      "    530       0.09775       1.42475      0.06861      0.78073  12.33s\n",
      "    531       0.08064       1.44685      0.05573      0.77277  12.37s\n",
      "    532       0.11444       1.39580      0.08199      0.77511  12.39s\n",
      "    533       \u001b[36m0.06198\u001b[0m       1.40552      0.04410      0.77433  12.37s\n",
      "    534       0.10287       1.44128      0.07137      0.77519  12.36s\n",
      "    535       0.07431       1.47083      0.05052      0.77292  12.36s\n",
      "    536       0.08062       1.46615      0.05499      0.77746  12.38s\n",
      "    537       0.09656       1.48356      0.06509      0.77831  12.34s\n",
      "    538       0.43289       1.38254      0.31311      0.78903  12.38s\n",
      "    539       0.06513       1.39233      0.04678      0.77113  12.35s\n",
      "    540       0.16955       1.37636      0.12319      0.77448  12.34s\n",
      "    541       0.13339       1.36697      0.09758      0.76659  12.36s\n",
      "    542       0.14927       1.36270      0.10954      0.76510  12.36s\n",
      "    543       0.13415       1.37056      0.09788      0.75551  12.32s\n",
      "    544       0.14128       1.35535      0.10424      0.76987  12.38s\n",
      "    545       0.10636       1.37804      0.07718      0.77519  12.35s\n",
      "    546       0.10815       1.43091      0.07558      0.77199  12.36s\n",
      "    547       0.12187       1.44016      0.08463      0.77277  12.36s\n",
      "    548       0.14999       1.40936      0.10643      0.78293  12.37s\n",
      "    549       0.22709       1.35352      0.16778      0.77362  12.36s\n",
      "    550       0.19820       1.30243      0.15218      0.77697  12.43s\n",
      "    551       0.16106       1.28145      0.12569      0.77612  12.46s\n",
      "    552       0.08780       1.37264      0.06397      0.77043  12.55s\n",
      "    553       0.26680       1.32867      0.20081      0.77340  12.58s\n",
      "    554       0.10684       1.36564      0.07823      0.76857  12.37s\n",
      "    555       0.12046       1.34965      0.08925      0.77021  12.35s\n",
      "    556       0.06520       1.39109      0.04687      0.76254  12.38s\n",
      "    557       0.08375       1.42129      0.05893      0.76261  12.37s\n",
      "    558       0.09913       1.39403      0.07111      0.76864  12.39s\n",
      "    559       0.06981       1.42450      0.04900      0.76481  12.35s\n",
      "    560       0.13522       1.41552      0.09553      0.76794  12.39s\n",
      "    561       \u001b[36m0.05228\u001b[0m       1.42345      0.03673      0.76623  12.35s\n",
      "    562       0.15558       1.42282      0.10935      0.76332  12.41s\n",
      "    563       0.08262       1.40231      0.05892      0.77050  12.34s\n",
      "    564       0.12026       1.36475      0.08812      0.77228  12.40s\n",
      "    565       0.06422       1.38704      0.04630      0.77853  12.39s\n",
      "    566       0.05392       1.41070      0.03822      0.77839  12.38s\n",
      "    567       0.10051       1.40628      0.07147      0.78222  12.40s\n",
      "    568       0.11129       1.44994      0.07675      0.77504  12.38s\n",
      "    569       0.11497       1.43265      0.08025      0.78051  12.40s\n",
      "    570       0.08688       1.46141      0.05945      0.78073  12.36s\n",
      "    571       0.17637       1.42866      0.12345      0.77284  12.38s\n",
      "    572       0.08096       1.42430      0.05684      0.77065  12.37s\n",
      "    573       0.12158       1.41274      0.08606      0.77455  12.40s\n",
      "    574       0.07586       1.39858      0.05424      0.76901  12.50s\n",
      "    575       0.15232       1.41203      0.10787      0.77660  12.55s\n",
      "    576       0.07746       1.47358      0.05256      0.77433  12.38s\n"
     ]
    }
   ],
   "source": [
    "# train nn\n",
    "net.load_params_from(os.path.join(model_root, 'recog_for_icdar.pkl')); # or load a pretrained model!\n",
    "net.fit(data_train_x, data_train_y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.781767955801\n"
     ]
    }
   ],
   "source": [
    "pred = net.predict(data_test_x)\n",
    "print accuracy_score(data_test_y, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          !       0.27      0.75      0.40         8\n",
      "          \"       0.00      0.00      0.00         1\n",
      "          &       1.00      0.14      0.25         7\n",
      "          '       0.25      0.12      0.17         8\n",
      "          (       0.00      0.00      0.00         1\n",
      "          )       1.00      1.00      1.00         1\n",
      "          ,       0.00      0.00      0.00         6\n",
      "          -       1.00      0.75      0.86         4\n",
      "          .       0.00      0.00      0.00        11\n",
      "          0       0.67      0.04      0.08        46\n",
      "          1       0.81      0.57      0.67        46\n",
      "          2       0.85      0.94      0.89        49\n",
      "          3       0.92      0.65      0.76        17\n",
      "          4       0.72      0.75      0.73        24\n",
      "          5       0.88      0.48      0.62        29\n",
      "          6       0.80      0.53      0.64        15\n",
      "          7       0.67      0.20      0.31        10\n",
      "          8       0.40      0.33      0.36         6\n",
      "          9       0.57      0.27      0.36        15\n",
      "          ?       0.00      0.00      0.00         1\n",
      "          A       0.97      0.86      0.91       223\n",
      "          B       0.68      0.81      0.74        47\n",
      "          C       0.84      0.83      0.83       153\n",
      "          D       0.69      0.80      0.74        74\n",
      "          E       0.87      0.92      0.90       322\n",
      "          F       0.86      0.93      0.89        76\n",
      "          G       0.98      0.86      0.92        63\n",
      "          H       0.89      0.92      0.90        97\n",
      "          I       0.66      0.40      0.50       163\n",
      "          J       0.45      0.38      0.42        13\n",
      "          K       1.00      0.61      0.76        46\n",
      "          L       0.79      0.82      0.80       131\n",
      "          M       0.80      0.83      0.82        89\n",
      "          N       1.00      0.80      0.89       153\n",
      "          O       0.65      0.81      0.72       187\n",
      "          P       0.79      0.90      0.84        91\n",
      "          Q       0.00      0.00      0.00         4\n",
      "          R       0.90      0.88      0.89       205\n",
      "          S       0.85      0.83      0.84       229\n",
      "          T       0.79      0.86      0.82       205\n",
      "          U       0.69      0.80      0.74        92\n",
      "          V       0.72      0.69      0.71        26\n",
      "          W       0.69      0.79      0.74        39\n",
      "          X       1.00      0.84      0.91        19\n",
      "          Y       0.90      0.88      0.89        42\n",
      "          Z       0.29      0.29      0.29         7\n",
      "          a       0.83      0.84      0.83       171\n",
      "          b       0.57      0.88      0.69        24\n",
      "          c       0.85      0.68      0.76       100\n",
      "          d       0.77      0.85      0.81        54\n",
      "          e       0.91      0.88      0.90       331\n",
      "          f       0.84      0.79      0.81        47\n",
      "          g       0.48      0.76      0.59        38\n",
      "          h       0.86      0.81      0.84        86\n",
      "          i       0.78      0.82      0.80       182\n",
      "          j       0.00      0.00      0.00         4\n",
      "          k       0.62      0.91      0.74        33\n",
      "          l       0.28      0.52      0.36       105\n",
      "          m       0.72      0.82      0.77        51\n",
      "          n       0.91      0.89      0.90       162\n",
      "          o       0.79      0.68      0.73       194\n",
      "          p       0.85      0.59      0.69        56\n",
      "          q       0.00      0.00      0.00         3\n",
      "          r       0.85      0.86      0.86       177\n",
      "          s       0.70      0.86      0.77       154\n",
      "          t       0.85      0.89      0.87       173\n",
      "          u       0.67      0.76      0.71        67\n",
      "          v       0.82      0.58      0.68        24\n",
      "          w       0.61      0.58      0.59        19\n",
      "          x       0.83      0.83      0.83        12\n",
      "          y       0.85      0.77      0.81        57\n",
      "          z       0.00      0.00      0.00         2\n",
      "          Â£       0.00      0.00      0.00         3\n",
      "\n",
      "avg / total       0.80      0.79      0.79      5430\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cuda/anaconda2/lib/python2.7/site-packages/sklearn/metrics/classification.py:958: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print classification_report(data_test_y, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "net.save_params_to(os.path.join(model_root, 'recog_for_icdar.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
